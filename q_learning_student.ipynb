{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Q-Learning \n",
    "\n",
    "Le TP vise à implémenter un algorithme d'apprentissage par renforcement.\n",
    "### Description : \n",
    "Un robot (ou agent) se déplace dans une grille 4x4. La grille comporte 3 cases particulières : un mur, un puit, et une arrivée. Si l’agent arrive sur le puit ou l’arrivée, le jeu se termine. L'agent ne peut pas se déplacer sur le mur, et le joueur est récompensé de -1 à chaque fois qu’il arrive sur une case non colorée.\n",
    "L’environnement peut être doté d’une composante aléatoire : à chaque choix de mouvement (haut, bas, gauche ou droite), l’agent pourra se déplacer dans une direction non voulue. Ce comportement est paramétrable. (Voir les explications supplémentaires au tableau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Game:\n",
    "    ACTION_UP = 0\n",
    "    ACTION_LEFT = 1\n",
    "    ACTION_DOWN = 2\n",
    "    ACTION_RIGHT = 3\n",
    "    \n",
    "    ACTIONS = [ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT]\n",
    "    ACTIONS_NAMES = ['UP','LEFT','DOWN','RIGHT']\n",
    "    \n",
    "    MOVEMENTS = {\n",
    "        ACTION_UP: (1, 0),\n",
    "        ACTION_RIGHT: (0, 1),\n",
    "        ACTION_LEFT: (0, -1),\n",
    "        ACTION_DOWN: (-1, 0)\n",
    "    }\n",
    "    \n",
    "    num_actions = len(ACTIONS)\n",
    "    \n",
    "    def __init__(self, n, m, wrong_action_p=0.1, alea=False):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.wrong_action_p = wrong_action_p\n",
    "        self.alea = alea\n",
    "        self.generate_game()\n",
    "        \n",
    "    def _position_to_id(self, x, y):\n",
    "        \"\"\"Donne l'identifiant id de la case id (de 0 à n)\"\"\"\n",
    "        return x + y * self.n\n",
    "    \n",
    "    def _id_to_position(self, id):\n",
    "        \"\"\"Réciproque de la fonction précédente\"\"\"\n",
    "        return (id % self.n, id // self.n)\n",
    "    \n",
    "    def generate_game(self):\n",
    "        cases = [(x, y) for x in range(self.n) for y in range(self.m)]\n",
    "        hole = random.choice(cases)\n",
    "        cases.remove(hole)\n",
    "        start = random.choice(cases)\n",
    "        cases.remove(start)\n",
    "        end = random.choice(cases)\n",
    "        cases.remove(end)\n",
    "        block = random.choice(cases)\n",
    "        cases.remove(block)\n",
    "        \n",
    "        self.position = start\n",
    "        self.end = end\n",
    "        self.hole = hole\n",
    "        self.block = block\n",
    "        self.counter = 0\n",
    "        \n",
    "        if not self.alea:\n",
    "            self.start = start\n",
    "        return self._get_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        if not self.alea:\n",
    "            self.position = self.start\n",
    "            self.counter = 0\n",
    "            return self._get_state()\n",
    "        else:\n",
    "            return self.generate_game() \n",
    "    \n",
    "    def _get_grille(self, x, y):\n",
    "        grille = [\n",
    "            [0] * self.n for i in range(self.m)\n",
    "        ]\n",
    "        grille[x][y] = 1\n",
    "        return grille\n",
    "    \n",
    "    def _get_state(self):\n",
    "        if self.alea:\n",
    "            return [self._get_grille(x, y) for (x, y) in\n",
    "                    [self.position, self.end, self.hole, self.block]]\n",
    "        return self._position_to_id(*self.position)\n",
    "   \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        prend l'action en parametètre\n",
    "        :param action : l'id de l'action\n",
    "        :retourne ((state_id, end, hole, block), reward, is_final, actions)\n",
    "        \"\"\"\n",
    "        self.counter += 1\n",
    "        if action not in self.ACTIONS:\n",
    "            raise Exception('Invalid action')\n",
    "        \n",
    "        choice = random.random()\n",
    "        if choice < self.wrong_action_p :\n",
    "            action = (action + 1) % 4\n",
    "        elif choice < 2 * self.wrong_action_p:\n",
    "            action = (action - 1) % 4\n",
    "            \n",
    "        d_x, d_y = self.MOVEMENTS[action]\n",
    "        x, y = self.position\n",
    "        new_x, new_y = x + d_x, y + d_y\n",
    "        \n",
    "        if self.block == (new_x, new_y):\n",
    "            return self._get_state(), -1, False, self.ACTIONS\n",
    "        elif self.hole == (new_x, new_y):\n",
    "            self.position = new_x, new_y\n",
    "            return self._get_state(), -10, True, None\n",
    "        elif self.end == (new_x, new_y):\n",
    "            self.position = new_x, new_y\n",
    "            return self._get_state(), 10, True, self.ACTIONS\n",
    "        elif new_x >= self.n or new_y >= self.m or new_x < 0 or new_y < 0:\n",
    "            return self._get_state(), -1, False, self.ACTIONS\n",
    "        elif self.counter > 190:\n",
    "            self.position = new_x, new_y\n",
    "            return self._get_state(), -10, True, self.ACTIONS\n",
    "        else:\n",
    "            self.position = new_x, new_y\n",
    "            return self._get_state(), -1, False, self.ACTIONS\n",
    "        \n",
    "    def print(self):\n",
    "        str = \"\"\n",
    "        for i in range(self.n - 1, -1, -1):\n",
    "            for j in range(self.m):\n",
    "                if (i, j) == self.position:\n",
    "                    str += \"x\"\n",
    "                elif (i, j) == self.block:\n",
    "                    str += \"¤\"\n",
    "                elif (i, j) == self.hole:\n",
    "                    str += \"o\"\n",
    "                elif (i, j) == self.end:\n",
    "                    str += \"@\"\n",
    "                else:\n",
    "                    str += \".\"\n",
    "            str += \"\\n\"\n",
    "        print(str)        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supose que l'environnement est fixe :\n",
    "\n",
    "la position des éléments ne sera pas modifiée entre chaque partie. Il s'agit donc d'apprendre la structure du terrain par coeur, pour pouvoir déplacer l’agent correctement.\n",
    "\n",
    "Dans le reste de ce TP, nous allons implémenter ensemble un algorithme de Q-Learning avec une table. \n",
    "\n",
    "Les formules mathématiques et la démarche à suivre est expliquée au tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "num_states = 16\n",
    "num_actions = 4\n",
    "Q = np.zeros([num_states, num_actions])\n",
    "lr = .85 # alpha\n",
    "y = .99 # gamma\n",
    "num_episodes = 1000\n",
    "cumul_reward_list = []\n",
    "actions_list = []\n",
    "states_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ox..\n",
      "...@\n",
      "....\n",
      "..¤.\n",
      "\n",
      "Score over time : 0.56\n",
      "[0, 2, 2, 0, 3, 3] [7, 6, 5, 6, 10, 14]\n",
      "o...\n",
      "...x\n",
      "....\n",
      "..¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game = Game(4, 4, 0)\n",
    "game.print()\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    actions = []\n",
    "    s = game.reset()\n",
    "    states = []\n",
    "    cumul_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        Q2 = Q[s, :]\n",
    "        a = np.argmax(Q2)\n",
    "        s1, reward, e, _ = game.move(a)\n",
    "        Q[s, a] = Q[s, a] + lr * (reward + y * np.max(Q[s, :]) - Q[s, a])\n",
    "        cumul_reward += reward\n",
    "        s = s1\n",
    "        actions.append(a)\n",
    "        states.append(s)\n",
    "        if e:\n",
    "            break\n",
    "    states_list.append(states)\n",
    "    actions_list.append(actions)\n",
    "    cumul_reward_list.append(cumul_reward)\n",
    "\n",
    "print(f\"Score over time : {sum(cumul_reward_list[-100:])/100.0}\")\n",
    "print(actions, states)\n",
    "game.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
